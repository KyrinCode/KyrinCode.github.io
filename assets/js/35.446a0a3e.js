(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{657:function(t,a,s){"use strict";s.r(a);var n=s(4),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("博一《深度学习》课程论文选读：WDSR（NTIRE2018 超分辨率冠军）阅读报告")]),t._v(" "),s("h2",{attrs:{id:"论文介绍"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#论文介绍"}},[t._v("#")]),t._v(" 论文介绍")]),t._v(" "),s("p",[t._v("本次论文阅读，选择的是计算机视觉里单图像超分辨率领域的一篇文章 WSDR。论文全称：Wide Activation for Efficient and Accurate Image Super-Resolution，是 NTIRE2018 超分辨率比赛的冠军。")]),t._v(" "),s("p",[t._v("论文地址："),s("a",{attrs:{href:"https://arxiv.org/abs/1808.08718",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://arxiv.org/abs/1808.08718"),s("OutboundLink")],1)]),t._v(" "),s("h2",{attrs:{id:"问题背景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#问题背景"}},[t._v("#")]),t._v(" 问题背景")]),t._v(" "),s("p",[t._v("超分辨率（Super-Resolution）就是：将低分辨率的图像通过算法转换成高分辨率图像。目前看来，基于深度学习的超分辨率算法已经达到了很好的效果。")]),t._v(" "),s("p",[t._v("超分辨率具有广泛的实际需求和应用场景，在数字成像、卫星遥感、目标识别和医学影像等方面，都会有使显示设备分辨率大于图像源分辨率的需求。")]),t._v(" "),s("p",[t._v("超分辨率领域有两个重要的两个评价指标：PSNR 峰值信噪比、SSIM 结构相似性。")]),t._v(" "),s("h2",{attrs:{id:"算法贡献"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#算法贡献"}},[t._v("#")]),t._v(" 算法贡献")]),t._v(" "),s("p",[t._v("WDSR 的算法是以 NTIRE2017 超分辨率冠军 EDSR 为 baseline 的，以 PSNR 峰值信噪比和直观显示上都比 EDSR 要强。网络结构如下：")]),t._v(" "),s("p",[t._v("![Network structure](./images/WDSR/Network structure.png)")]),t._v(" "),s("p",[t._v("左侧为 baseline EDSR 网络，右侧为该论文中提出的 WDSR 网络。相比于 EDSR，WDSR 做出了几个改变。")]),t._v(" "),s("p",[s("strong",[t._v("结构上的提升")]),t._v("：")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("去除了很多冗余的卷积层（上图阴影部分），使计算更快，作者认为这些多余的卷积层的效果可以包含在 Resblock 中。在去除实验后，发现效果并没有下降，所以去除冗余的卷积层可以降低计算开销。")])]),t._v(" "),s("li",[s("p",[t._v("改造了 Resblock，下图中左图为 EDSR 的 Resblock，右侧两个为本文提出的两种 WDSR 结构。这两种 WDSR 仅在 Resblock 中不同，实际效果也差不多。对比 WDSR-A 和 EDSR 中的 Resblock，WDSR-A 在不增加计算开销的前提下，增加 ReLu 前卷积核的卷积核数以增加输出特征的宽度。WDSR-B 进一步解放了计算开销，将 ReLu后的卷积核拆分成两个小卷积核，这样可以在同样计算开销的前提下获得更宽泛的激活函数前的特征图。")])])]),t._v(" "),s("p",[t._v("![Residual structure](./images/WDSR/Residual structure.png)")]),t._v(" "),s("p",[s("strong",[t._v("加入权重标准化 Weight Normalization")]),t._v("：")]),t._v(" "),s("p",[t._v("深度学习中有很多归一化的方式，是为了保证每层网络的数据输入分布从而加快网络的收敛。本文选择 WN 的原因是，实验中发现，加入 WN 虽然不能明显提高结果的准确性，但是加入后可以使用10倍以上更高的学习率，加速训练。")]),t._v(" "),s("p",[t._v("总的来说，WDSR 具有以下创新点：")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("提高了 ReLu 层前的核数；")])]),t._v(" "),s("li",[s("p",[t._v("节省卷积数量，优化结构，在同参数的情况下提高准确率；")])]),t._v(" "),s("li",[s("p",[t._v("加入权重标准化 WN，加速训练。")])])]),t._v(" "),s("h2",{attrs:{id:"阅后思考"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#阅后思考"}},[t._v("#")]),t._v(" 阅后思考")]),t._v(" "),s("p",[t._v("基于深度学习的 SR 算法目前有基于 CNN 的和基于 GAN 的。但在该领域还是基于 CNN 的方法比基于 GAN 的效果更好。")]),t._v(" "),s("p",[t._v("在研究超分辨率算法时，有几种关键的技术做支撑。")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("深度神经网络或循环神经网络。在 SR 算法中，神经网络的深度是影响SR算法性能的关键因素之一。")])]),t._v(" "),s("li",[s("p",[t._v("上采样方法。在低分辨率图像转换到高分辨率图像过程中，需要一种上采样方法。早期 SR 算法使用的是反卷积 deconvolution。如果直接用deconvolution，通常会带入过多人工因素。一种叫 pixel shuffle 的新型卷积方法也叫 sub-pixel convolution 在 CVPR2016 年被提出来。这种新型卷积就是为 SR 量身定做的，它将输出的多个特征图像素级的重新拼接起来，实际证明效果确实更好。")])]),t._v(" "),s("li",[s("p",[t._v("跳跃连接。如残差网络 Resnet，将前层输出与深层输出相加连接训练边缘，可以有效利用浅层特征信息，还可以有效解决反向传播的梯度弥散。目前主流 SR 算法都会有 Resblock。")])]),t._v(" "),s("li",[s("p",[t._v("标准化层。深度学习中有很多归一化的方式，是为了保证每层网络的数据输入分布从而加快网络的收敛。")])])]),t._v(" "),s("h2",{attrs:{id:"bibtex"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#bibtex"}},[t._v("#")]),t._v(" BibTeX")]),t._v(" "),s("div",{staticClass:"language-latex line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-latex"}},[s("code",[t._v("@article"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("Yu2018WideAF,\n  title="),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("Wide Activation for Efficient and Accurate Image Super-Resolution"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n  author="),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("Jiahui Yu and Yuchen Fan and Jianchao Yang and Ning Xu and Zhaowen Wang and Xinchao Wang and Thomas S. Huang"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n  journal="),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("ArXiv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n  year="),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("2018"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n  volume="),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("abs/1808.08718"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br")])])])}),[],!1,null,null,null);a.default=e.exports}}]);